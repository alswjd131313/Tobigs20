{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basic Assignment\n",
    "## 과제 : spam.csv를 활용하여 유의미한 해석을 도출해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "- 보시면 아시다시피 spam.csv는 라벨이 있는 데이터입니다.\n",
    "- 7주차 주제가 텍스트 기초인만큼 텍스트만 활용하셔도 되고 라벨까지 활용하셔서 모델을 돌려보셔도 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.iloc[5]['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.iloc[8]['v2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코퍼스: 분석에 활용하기 위한 자연어 데이터\n",
    "- 토큰: 코퍼스를 의미 있는 작은 단위로 나누었을 때의 이 단위\n",
    "- 토큰화: 하나의 코퍼스를 여러 개의 토큰으로 나누는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고> https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MJHwang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FreeMsg',\n",
       " 'Hey',\n",
       " 'there',\n",
       " 'darling',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'been',\n",
       " '3',\n",
       " 'week',\n",
       " \"'s\",\n",
       " 'now',\n",
       " 'and',\n",
       " 'no',\n",
       " 'word',\n",
       " 'back',\n",
       " '!',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'you',\n",
       " 'up',\n",
       " 'for',\n",
       " 'it',\n",
       " 'still',\n",
       " '?',\n",
       " 'Tb',\n",
       " 'ok',\n",
       " '!',\n",
       " 'XxX',\n",
       " 'std',\n",
       " 'chgs',\n",
       " 'to',\n",
       " 'send',\n",
       " ',',\n",
       " 'å£1.50',\n",
       " 'to',\n",
       " 'rcv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 코드 코드\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "word_tokenize(spam.iloc[5]['v2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_tokenize : 어퍼스트로피나 콤마를 토큰화의 기준으로 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"FreeMsg Hey there darling it's been 3 week's now and no word back!\",\n",
       " \"I'd like some fun you up for it still?\",\n",
       " 'Tb ok!',\n",
       " 'XxX std chgs to send, å£1.50 to rcv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(spam.iloc[5]['v2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sent_tokenize : 문장 단위로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7),\n",
       " (8, 11),\n",
       " (12, 17),\n",
       " (18, 25),\n",
       " (26, 30),\n",
       " (31, 35),\n",
       " (36, 37),\n",
       " (38, 44),\n",
       " (45, 48),\n",
       " (49, 52),\n",
       " (53, 55),\n",
       " (56, 60),\n",
       " (61, 66),\n",
       " (67, 70),\n",
       " (71, 75),\n",
       " (76, 80),\n",
       " (81, 84),\n",
       " (85, 88),\n",
       " (89, 91),\n",
       " (92, 95),\n",
       " (96, 98),\n",
       " (99, 105),\n",
       " (106, 108),\n",
       " (109, 112),\n",
       " (113, 116),\n",
       " (117, 120),\n",
       " (121, 125),\n",
       " (126, 128),\n",
       " (129, 134),\n",
       " (135, 141),\n",
       " (142, 144),\n",
       " (145, 148)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "list(WhitespaceTokenizer().span_tokenize(spam.iloc[5]['v2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "- 수업에서 다룬 임베딩 방법에는 One-hot encoding, CBOW, Skip-gram 등이 있었습니다. 다양한 시도와 '비교' 결과를 함께 적어주세요! 파라미터를 조정해가는 과정도 해석에 도움이 될 수 있겠죠 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'FreeMsg': 1,\n",
       "         'Hey': 1,\n",
       "         'there': 1,\n",
       "         'darling': 1,\n",
       "         'it': 2,\n",
       "         \"'s\": 2,\n",
       "         'been': 1,\n",
       "         '3': 1,\n",
       "         'week': 1,\n",
       "         'now': 1,\n",
       "         'and': 1,\n",
       "         'no': 1,\n",
       "         'word': 1,\n",
       "         'back': 1,\n",
       "         '!': 2,\n",
       "         'I': 1,\n",
       "         \"'d\": 1,\n",
       "         'like': 1,\n",
       "         'some': 1,\n",
       "         'fun': 1,\n",
       "         'you': 1,\n",
       "         'up': 1,\n",
       "         'for': 1,\n",
       "         'still': 1,\n",
       "         '?': 1,\n",
       "         'Tb': 1,\n",
       "         'ok': 1,\n",
       "         'XxX': 1,\n",
       "         'std': 1,\n",
       "         'chgs': 1,\n",
       "         'to': 2,\n",
       "         'send': 1,\n",
       "         ',': 1,\n",
       "         'å£1.50': 1,\n",
       "         'rcv': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokenized_word = word_tokenize(spam.iloc[5]['v2'])\n",
    "vocab = Counter(tokenized_word)\n",
    "vocab\n",
    "\n",
    "# counter : 파라미터를 단어 리스트로 받고, 각 단어의 등장 빈도를 딕셔너리 형태로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합:  {'to': 1, 'freemsg': 2, 'hey': 3, 'there': 4, 'darling': 5, \"it's\": 6, 'been': 7, '3': 8, \"week's\": 9, 'now': 10, 'and': 11, 'no': 12, 'word': 13, 'back': 14, \"i'd\": 15, 'like': 16, 'some': 17, 'fun': 18, 'you': 19, 'up': 20, 'for': 21, 'it': 22, 'still': 23, 'tb': 24, 'ok': 25, 'xxx': 26, 'std': 27, 'chgs': 28, 'send': 29, 'å£1': 30, '50': 31, 'rcv': 32}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([spam.iloc[5]['v2']])\n",
    "\n",
    "print(\"단어 집합: \", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 1, 29, 30, 31, 1, 32]]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences([spam.iloc[5]['v2']])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 0. 1. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 1. 0.]\n",
      "   [0. 1. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical([encoded])\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 1, 29, 30, 31, 1, 32] 을 순서대로 one-hot encoding이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot 인코딩은 희소표현. cbow, skip-gram은 분산표현\n",
    "- 희소 표현: 표현하고자 하는 단어의 인덱스 값만 1, 나머지는 0. 각 단어간 유사성을 표현할 수 없음.\n",
    "<br>\n",
    "ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0]\n",
    "- 분산 표현: 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현. 단어 간 유사도 계산 가능.\n",
    "<br>\n",
    "ex) 강아지 = [0.2 0.3 0.5 0.7 0.2 ... 중략 ... 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FreeMsg',\n",
       " 'Hey',\n",
       " 'there',\n",
       " 'darling',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'been',\n",
       " '3',\n",
       " 'week',\n",
       " \"'s\",\n",
       " 'now',\n",
       " 'and',\n",
       " 'no',\n",
       " 'word',\n",
       " 'back',\n",
       " '!',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'you',\n",
       " 'up',\n",
       " 'for',\n",
       " 'it',\n",
       " 'still',\n",
       " '?',\n",
       " 'Tb',\n",
       " 'ok',\n",
       " '!',\n",
       " 'XxX',\n",
       " 'std',\n",
       " 'chgs',\n",
       " 'to',\n",
       " 'send',\n",
       " ',',\n",
       " 'å£1.50',\n",
       " 'to',\n",
       " 'rcv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec의 sg에 따라 cbow, skip-gram 선택 가능\n",
    "- sg = 0 -> cbow\n",
    "- sg = 1 -> skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x25b12369ca0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "cbow = Word2Vec(sentences=spam['v2'].apply(word_tokenize), vector_size=100, window=5, min_count=3, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Word2Vec(sentences=spam['v2'].apply(word_tokenize), vector_size=100, window=5, min_count=3, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 본인이 도출해낸 해석을 적어주세요!\n",
    "\n",
    "- 유사도, Wordcloud, 이진 분류 모델, Plot 뭐든 상관없으니 분명하고 인상적인 해석을 적어주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 단어 간 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between word1 and word2: 0.99920595\n"
     ]
    }
   ],
   "source": [
    "similarity_score = cbow.wv.similarity(\"week\", \"every\")\n",
    "print(\"Similarity between word1 and word2:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between word1 and word2: 0.90321475\n"
     ]
    }
   ],
   "source": [
    "similarity_score = skipgram.wv.similarity(\"week\", \"every\")\n",
    "print(\"Similarity between word1 and word2:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어 간 가장 유사한 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ur', 0.9993053078651428),\n",
       " ('every', 0.9992058873176575),\n",
       " ('stop', 0.9991354942321777),\n",
       " ('Get', 0.9991350173950195),\n",
       " ('Good', 0.9990766644477844),\n",
       " ('only', 0.9990458488464355),\n",
       " ('free', 0.9989662766456604),\n",
       " ('Dear', 0.998926043510437),\n",
       " ('new', 0.9988760352134705),\n",
       " ('8007', 0.9988464713096619)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv.most_similar('week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('word', 0.9381897449493408),\n",
       " ('new', 0.9144180417060852),\n",
       " ('For', 0.911250114440918),\n",
       " ('video', 0.9106596112251282),\n",
       " ('every', 0.903214693069458),\n",
       " ('UK', 0.8949734568595886),\n",
       " ('SMS', 0.8920695185661316),\n",
       " ('NOKIA', 0.8890995979309082),\n",
       " ('Get', 0.8816767334938049),\n",
       " ('collect', 0.8803457021713257)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.wv.most_similar('week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 단어 벡터 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'word': [-1.42568186e-01  2.31209040e-01  1.07032202e-01  4.81613502e-02\n",
      "  4.53250632e-02 -4.29760665e-01  1.68147832e-01  5.32940090e-01\n",
      " -2.58000195e-01 -1.65584818e-01 -7.27882311e-02 -3.38820040e-01\n",
      "  1.08320631e-01  8.02676156e-02  1.22988231e-01 -1.58413351e-01\n",
      "  2.04646647e-01 -2.23385870e-01 -6.67287633e-02 -4.94882107e-01\n",
      "  1.21062405e-01  7.23120347e-02  1.10856242e-01 -1.40261769e-01\n",
      " -8.57421681e-02 -2.73279026e-02 -2.55403399e-01 -1.23022057e-01\n",
      " -2.70551592e-01  1.24705732e-02  2.31581196e-01  2.03479901e-02\n",
      "  1.20088384e-01 -2.73764640e-01 -5.91451600e-02  2.30480075e-01\n",
      " -5.60998963e-03 -1.51757821e-01 -2.27854699e-01 -4.11120504e-01\n",
      "  1.30755976e-02 -2.46428832e-01 -1.27879292e-01  8.72472599e-02\n",
      "  1.93697870e-01 -1.87982783e-01 -1.48701414e-01 -1.09842546e-01\n",
      "  8.21743831e-02  1.68384939e-01  2.24017963e-01 -3.43009412e-01\n",
      " -7.08083287e-02 -4.81659099e-02 -7.14578032e-02  7.67880976e-02\n",
      "  1.47160843e-01 -6.14247052e-03 -3.46115619e-01  7.23671690e-02\n",
      "  1.16487294e-01  8.05881023e-02  6.43573478e-02 -3.26848738e-02\n",
      " -1.84219107e-01  3.16546947e-01  1.16294801e-01  2.94243336e-01\n",
      " -3.41730028e-01  3.11083376e-01  1.08998816e-03  1.45644277e-01\n",
      "  2.57305413e-01  2.95023266e-02  3.10872078e-01  3.48376185e-02\n",
      "  6.85084462e-02 -1.62935071e-02 -1.80853441e-01 -7.27823973e-02\n",
      " -2.41772011e-01 -1.96615886e-02 -3.04606110e-01  3.45784396e-01\n",
      " -1.48581088e-01 -7.30981454e-02  1.70580566e-01  2.54573077e-01\n",
      "  2.34915257e-01  1.12722389e-01  3.06999177e-01  1.27611548e-01\n",
      "  5.00874259e-02  1.71268731e-02  3.78719807e-01  6.84315562e-02\n",
      "  5.25127016e-02 -1.90477863e-01 -4.39716736e-04 -5.22146374e-02]\n"
     ]
    }
   ],
   "source": [
    "vector = cbow.wv[\"word\"]\n",
    "print(\"Vector for 'word':\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'word': [-0.03196335  0.10805499  0.19064097  0.10633983 -0.04288951 -0.36377376\n",
      "  0.20999326  0.34201235 -0.21890335 -0.04363089 -0.03080299 -0.34259453\n",
      " -0.13266449  0.13810052 -0.10727    -0.01850996  0.22304724 -0.19274415\n",
      "  0.01843401 -0.4310174   0.20201294 -0.07779159  0.04901658 -0.03003365\n",
      " -0.10303938 -0.10883077 -0.21747072 -0.17795484 -0.2035624   0.0131456\n",
      "  0.23820229  0.15389147  0.09024838 -0.2658937  -0.0053693  -0.06856731\n",
      " -0.00578836 -0.29147112 -0.12758096 -0.30546114  0.02306102 -0.08499873\n",
      "  0.13350525 -0.1040848   0.0955472  -0.10975656 -0.07003711 -0.11428246\n",
      "  0.08415549  0.27035     0.11713465 -0.31333873 -0.21494344  0.06611905\n",
      " -0.01526462  0.24918462  0.13923153 -0.00556122 -0.3797307   0.04003341\n",
      "  0.1880127   0.2078167  -0.16021156 -0.10276476 -0.09697908  0.35390842\n",
      "  0.0323716   0.1375962  -0.19182122  0.22714925 -0.00261581  0.06428365\n",
      "  0.07657789  0.05006001  0.21826224 -0.03240235  0.13551939 -0.1028408\n",
      " -0.21210921 -0.16377506 -0.36428377  0.01358623 -0.3025864   0.28367105\n",
      " -0.02951987  0.00239617  0.00450255  0.09491897  0.10683831 -0.04745124\n",
      "  0.10986067 -0.0339711  -0.01353247  0.09054766  0.26759842  0.01812284\n",
      " -0.02199894 -0.19672807 -0.07227615  0.1891321 ]\n"
     ]
    }
   ],
   "source": [
    "vector = skipgram.wv[\"word\"]\n",
    "print(\"Vector for 'word':\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
